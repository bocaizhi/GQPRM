{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_evaluator import TransactionEvaluator\n",
    "from networkx.algorithms import isomorphism\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_and_success_rate(Q, full_transaction_history):\n",
    "    \"\"\"\n",
    "    计算查询图 Q 中每个顶点和边的交易频率和成功率\n",
    "\n",
    "    参数：\n",
    "    - Q: 查询图（networkx.Graph）\n",
    "    - full_transaction_history: List of transaction records，每个元素为 dict，含有：\n",
    "        {\n",
    "            'graph': G,            # networkx.Graph\n",
    "            'price': float,\n",
    "            'success': bool\n",
    "        }\n",
    "\n",
    "    返回：\n",
    "    - vertex_freq: dict[node] = frequency\n",
    "    - vertex_success: dict[node] = success rate\n",
    "    - edge_freq: dict[edge] = frequency\n",
    "    - edge_success: dict[edge] = success rate\n",
    "    \"\"\"\n",
    "    vertex_freq = defaultdict(int)\n",
    "    vertex_success = defaultdict(int)\n",
    "    edge_freq = defaultdict(int)\n",
    "    edge_success = defaultdict(int)\n",
    "\n",
    "    # 将 Q 的节点和边集合取出\n",
    "    query_nodes = set(Q.nodes())\n",
    "    query_edges = set(Q.edges())\n",
    "\n",
    "    # 遍历所有历史交易\n",
    "    for record in full_transaction_history:\n",
    "        G = record['graph']\n",
    "        success = record['success']\n",
    "\n",
    "        # 统计与 Q 交集部分的节点\n",
    "        matched_nodes = set(G.nodes()).intersection(query_nodes)\n",
    "        for node in matched_nodes:\n",
    "            vertex_freq[node] += 1\n",
    "            if success:\n",
    "                vertex_success[node] += 1\n",
    "\n",
    "        # 统计与 Q 交集部分的边（无向图考虑无向等价）\n",
    "        matched_edges = set()\n",
    "        for (u, v) in G.edges():\n",
    "            if (u in query_nodes) and (v in query_nodes):\n",
    "                if (u, v) in query_edges or (v, u) in query_edges:\n",
    "                    matched_edges.add((u, v))\n",
    "\n",
    "        for (u, v) in matched_edges:\n",
    "            edge_freq[(u, v)] += 1\n",
    "            if success:\n",
    "                edge_success[(u, v)] += 1\n",
    "\n",
    "    # 计算成功率\n",
    "    vertex_success_rate = {node: vertex_success[node] / vertex_freq[node]\n",
    "                           for node in vertex_freq}\n",
    "    edge_success_rate = {edge: edge_success[edge] / edge_freq[edge]\n",
    "                         for edge in edge_freq}\n",
    "\n",
    "    return vertex_freq, vertex_success_rate, edge_freq, edge_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centrality_for_query(Q, centrality_file):\n",
    "    \"\"\"\n",
    "    从中心性文件中提取图 Q 中顶点和边的中心性\n",
    "\n",
    "    参数：\n",
    "    - Q: 查询图（networkx.Graph）\n",
    "    - centrality_file: 中心性记录文件（pickle 格式，包含 vertex_centrality 和 edge_centrality）\n",
    "\n",
    "    返回：\n",
    "    - vertex_centrality_q: dict[node] = centrality 值\n",
    "    - edge_centrality_q: dict[(u, v)] = centrality 值\n",
    "    \"\"\"\n",
    "    with open(centrality_file, 'rb') as f:\n",
    "        vertex_centrality_all, edge_centrality_all = pickle.load(f)\n",
    "\n",
    "    vertex_centrality_q = {}\n",
    "    for node in Q.nodes():\n",
    "        if node in vertex_centrality_all:\n",
    "            vertex_centrality_q[node] = vertex_centrality_all[node]\n",
    "        else:\n",
    "            vertex_centrality_q[node] = 0  # 若不存在，默认中心性为0\n",
    "\n",
    "    edge_centrality_q = {}\n",
    "    for u, v in Q.edges():\n",
    "        if (u, v) in edge_centrality_all:\n",
    "            edge_centrality_q[(u, v)] = edge_centrality_all[(u, v)]\n",
    "        elif (v, u) in edge_centrality_all:  # 无向边\n",
    "            edge_centrality_q[(u, v)] = edge_centrality_all[(v, u)]\n",
    "        else:\n",
    "            edge_centrality_q[(u, v)] = 0  # 若不存在，默认中心性为0\n",
    "\n",
    "    return vertex_centrality_q, edge_centrality_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_weight_method(data_dict):\n",
    "    \"\"\"\n",
    "    使用熵值法计算指标的权重（适用于交易频率和成功率）\n",
    "\n",
    "    参数：\n",
    "    - data_dict: dict，格式为 {item_id: {'freq': ..., 'succ_rate': ...}}\n",
    "\n",
    "    返回：\n",
    "    - weight_dict: dict，格式为 {item_id: 权重值}\n",
    "    \"\"\"\n",
    "    if not data_dict:\n",
    "        return {}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='index')  # index=item_id, columns=['freq', 'succ_rate']\n",
    "    \n",
    "    # 若全部为 0，则避免除0错误\n",
    "    if (df.sum().sum() == 0):\n",
    "        return {item: 0 for item in data_dict}\n",
    "    \n",
    "    # 正向指标标准化（极小-极大标准化）\n",
    "    norm_df = (df - df.min()) / (df.max() - df.min() + 1e-12)  # 加小常数防止除以0\n",
    "    \n",
    "    # 计算每列的熵\n",
    "    eps = 1e-12\n",
    "    P = norm_df / (norm_df.sum(axis=0) + eps)\n",
    "    entropy = -np.sum(P * np.log(P + eps), axis=0) / np.log(len(df))\n",
    "\n",
    "    # 计算冗余度与权重\n",
    "    redundancy = 1 - entropy\n",
    "    weights = redundancy / np.sum(redundancy)\n",
    "\n",
    "    # 计算每个 item 的加权得分作为最终权重\n",
    "    final_scores = norm_df @ weights\n",
    "    weight_dict = dict(zip(df.index, final_scores))\n",
    "\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertex_entropy_weights(vertex_freq_dict, vertex_succ_dict):\n",
    "    \"\"\"\n",
    "    对顶点进行熵值加权计算\n",
    "    输入:\n",
    "        vertex_freq_dict: {node: freq}\n",
    "        vertex_succ_dict: {node: succ_rate}\n",
    "    输出:\n",
    "        {node: weight}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for node in set(vertex_freq_dict) | set(vertex_succ_dict):\n",
    "        data[node] = {\n",
    "            'freq': vertex_freq_dict.get(node, 0),\n",
    "            'succ_rate': vertex_succ_dict.get(node, 0)\n",
    "        }\n",
    "    return entropy_weight_method(data)\n",
    "\n",
    "\n",
    "def compute_edge_entropy_weights(edge_freq_dict, edge_succ_dict):\n",
    "    \"\"\"\n",
    "    对边进行熵值加权计算\n",
    "    输入:\n",
    "        edge_freq_dict: {(u, v): freq}\n",
    "        edge_succ_dict: {(u, v): succ_rate}\n",
    "    输出:\n",
    "        {(u, v): weight}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for edge in set(edge_freq_dict) | set(edge_succ_dict):\n",
    "        data[edge] = {\n",
    "            'freq': edge_freq_dict.get(edge, 0),\n",
    "            'succ_rate': edge_succ_dict.get(edge, 0)\n",
    "        }\n",
    "    return entropy_weight_method(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prices_advanced(\n",
    "    query_graph,\n",
    "    vertex_price_list,\n",
    "    edge_price_list,\n",
    "    new_price,\n",
    "    centrality_file,\n",
    "    full_transaction_history,\n",
    "    centrality_weight=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    优化后的价格更新函数：结合熵值法和中心性权重调整价格（动态计算频率与成功率）\n",
    "\n",
    "    :param query_graph: 查询图（networkx.Graph）\n",
    "    :param vertex_price_list: 顶点价格字典 {name: {'cost': c, 'price': p}}\n",
    "    :param edge_price_list: 边价格字典 {(u, v): {'weight': c, 'price': p}}\n",
    "    :param new_price: 目标总价格\n",
    "    :param centrality_file: pickle 文件路径，包含所有顶点和边的中心性值\n",
    "    :param full_transaction_history: 历史交易记录列表，每条记录是 {'graph': G, 'price': float, 'success': bool}\n",
    "    :param centrality_weight: 中心性权重（经验设置）\n",
    "    \"\"\"\n",
    "    def normalize(d):\n",
    "        total = sum(d.values()) or 1e-6\n",
    "        return {k: v / total for k, v in d.items()}\n",
    "\n",
    "    # === 中心性提取 ===\n",
    "    vertex_centrality_all, edge_centrality_all = get_centrality_for_query(query_graph, centrality_file)\n",
    "\n",
    "    used_vertices = set()\n",
    "    used_edges = set()\n",
    "    total_cost = 0\n",
    "\n",
    "    # === 获取图中实际使用的顶点和边 ===\n",
    "    for node in query_graph.nodes:\n",
    "        name = query_graph.nodes[node].get('name', node)\n",
    "        if name in vertex_price_list:\n",
    "            used_vertices.add(name)\n",
    "            total_cost += vertex_price_list[name]['cost']\n",
    "\n",
    "    for u, v in query_graph.edges:\n",
    "        name_u = query_graph.nodes[u].get('name', u)\n",
    "        name_v = query_graph.nodes[v].get('name', v)\n",
    "        edge_key = tuple(sorted((name_u, name_v)))\n",
    "        if edge_key in edge_price_list:\n",
    "            used_edges.add(edge_key)\n",
    "            total_cost += edge_price_list[edge_key]['weight']\n",
    "\n",
    "    # === fallback：按成本等比缩放 ===\n",
    "    if total_cost >= new_price:\n",
    "        adjustment_ratio = new_price / total_cost\n",
    "        for name in used_vertices:\n",
    "            cost = vertex_price_list[name]['cost']\n",
    "            vertex_price_list[name]['price'] = round(cost * adjustment_ratio, 4)\n",
    "        for edge in used_edges:\n",
    "            weight = edge_price_list[edge]['weight']\n",
    "            edge_price_list[edge]['price'] = round(weight * adjustment_ratio, 4)\n",
    "        return vertex_price_list, edge_price_list\n",
    "\n",
    "    # === 动态计算频率与成功率 ===\n",
    "    vertex_freq_dict, vertex_succ_dict, edge_freq_dict, edge_succ_dict = compute_frequency_and_success_rate(\n",
    "        query_graph, full_transaction_history\n",
    "    )\n",
    "\n",
    "    # === 熵值法权重 ===\n",
    "    v_weights = compute_vertex_entropy_weights(\n",
    "        {k: vertex_freq_dict.get(k, 0) for k in used_vertices},\n",
    "        {k: vertex_succ_dict.get(k, 0) for k in used_vertices}\n",
    "    )\n",
    "    e_weights = compute_edge_entropy_weights(\n",
    "        {k: edge_freq_dict.get(k, 0) for k in used_edges},\n",
    "        {k: edge_succ_dict.get(k, 0) for k in used_edges}\n",
    "    )\n",
    "\n",
    "    # === 提取中心性 ===\n",
    "    v_center = {k: vertex_centrality_all.get(k, 0) for k in used_vertices}\n",
    "    e_center = {k: edge_centrality_all.get(k, 0) for k in used_edges}\n",
    "\n",
    "    # === 归一化 ===\n",
    "    v_weights_norm = normalize(v_weights)\n",
    "    e_weights_norm = normalize(e_weights)\n",
    "    v_center_norm = normalize(v_center)\n",
    "    e_center_norm = normalize(e_center)\n",
    "\n",
    "    # === 组合权重 ===\n",
    "    v_combined = {\n",
    "        k: (1 - centrality_weight) * v_weights_norm.get(k, 0) + centrality_weight * v_center_norm.get(k, 0)\n",
    "        for k in used_vertices\n",
    "    }\n",
    "    e_combined = {\n",
    "        k: (1 - centrality_weight) * e_weights_norm.get(k, 0) + centrality_weight * e_center_norm.get(k, 0)\n",
    "        for k in used_edges\n",
    "    }\n",
    "\n",
    "    # === 分配价格 ===\n",
    "    remaining = new_price - total_cost\n",
    "    total_combined = sum(v_combined.values()) + sum(e_combined.values()) or 1e-6\n",
    "    v_ratio = {k: v / total_combined for k, v in v_combined.items()}\n",
    "    e_ratio = {k: v / total_combined for k, v in e_combined.items()}\n",
    "\n",
    "    for name in used_vertices:\n",
    "        base_cost = vertex_price_list[name]['cost']\n",
    "        alloc = remaining * v_ratio.get(name, 0)\n",
    "        vertex_price_list[name]['price'] = round(base_cost + alloc, 4)\n",
    "\n",
    "    for edge in used_edges:\n",
    "        base_cost = edge_price_list[edge]['weight']\n",
    "        alloc = remaining * e_ratio.get(edge, 0)\n",
    "        edge_price_list[edge]['price'] = round(base_cost + alloc, 4)\n",
    "\n",
    "    return vertex_price_list, edge_price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prices_advanced(\n",
    "    query_graph,\n",
    "    vertex_price_list,\n",
    "    edge_price_list,\n",
    "    vertex_freq_dict,\n",
    "    vertex_succ_dict,\n",
    "    edge_freq_dict,\n",
    "    edge_succ_dict,\n",
    "    new_price,\n",
    "    centrality_file,\n",
    "    centrality_weight=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    优化后的价格更新函数：结合熵值法和中心性权重调整价格（中心性自动提取）\n",
    "\n",
    "    :param query_graph: 查询图\n",
    "    :param vertex_price_list: 顶点价格字典 {name: {'cost': c, 'price': p}}\n",
    "    :param edge_price_list: 边价格字典 {(u, v): {'weight': c, 'price': p}}\n",
    "    :param vertex_freq_dict / vertex_succ_dict: 顶点频率/成功率字典\n",
    "    :param edge_freq_dict / edge_succ_dict: 边频率/成功率字典\n",
    "    :param new_price: 目标总价格\n",
    "    :param centrality_file: pickle 文件路径，包含所有顶点和边的中心性值\n",
    "    :param centrality_weight: 中心性权重（主观经验权重）\n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(d):\n",
    "        total = sum(d.values()) or 1e-6\n",
    "        return {k: v / total for k, v in d.items()}\n",
    "\n",
    "    # === 中心性提取 ===\n",
    "    vertex_centrality_all, edge_centrality_all = get_centrality_for_query(query_graph, centrality_file)\n",
    "\n",
    "    used_vertices = set()\n",
    "    used_edges = set()\n",
    "    total_cost = 0\n",
    "\n",
    "    # === 获取图中实际使用的顶点和边 ===\n",
    "    for node in query_graph.nodes:\n",
    "        name = query_graph.nodes[node].get('name', node)\n",
    "        if name in vertex_price_list:\n",
    "            used_vertices.add(name)\n",
    "            total_cost += vertex_price_list[name]['cost']\n",
    "\n",
    "    for u, v in query_graph.edges:\n",
    "        name_u = query_graph.nodes[u].get('name', u)\n",
    "        name_v = query_graph.nodes[v].get('name', v)\n",
    "        edge_key = tuple(sorted((name_u, name_v)))\n",
    "        if edge_key in edge_price_list:\n",
    "            used_edges.add(edge_key)\n",
    "            total_cost += edge_price_list[edge_key]['weight']\n",
    "\n",
    "    # === fallback：按成本等比缩放 ===\n",
    "    if total_cost >= new_price:\n",
    "        adjustment_ratio = new_price / total_cost\n",
    "        for name in used_vertices:\n",
    "            cost = vertex_price_list[name]['cost']\n",
    "            vertex_price_list[name]['price'] = round(cost * adjustment_ratio, 4)\n",
    "        for edge in used_edges:\n",
    "            weight = edge_price_list[edge]['weight']\n",
    "            edge_price_list[edge]['price'] = round(weight * adjustment_ratio, 4)\n",
    "        return vertex_price_list, edge_price_list\n",
    "\n",
    "    # === 熵值法权重 ===\n",
    "    v_weights = compute_vertex_entropy_weights(\n",
    "        {k: vertex_freq_dict.get(k, 0) for k in used_vertices},\n",
    "        {k: vertex_succ_dict.get(k, 0) for k in used_vertices}\n",
    "    )\n",
    "    e_weights = compute_edge_entropy_weights(\n",
    "        {k: edge_freq_dict.get(k, 0) for k in used_edges},\n",
    "        {k: edge_succ_dict.get(k, 0) for k in used_edges}\n",
    "    )\n",
    "\n",
    "    # === 提取中心性 ===\n",
    "    v_center = {k: vertex_centrality_all.get(k, 0) for k in used_vertices}\n",
    "    e_center = {k: edge_centrality_all.get(k, 0) for k in used_edges}\n",
    "\n",
    "    # === 归一化 ===\n",
    "    v_weights_norm = normalize(v_weights)\n",
    "    e_weights_norm = normalize(e_weights)\n",
    "    v_center_norm = normalize(v_center)\n",
    "    e_center_norm = normalize(e_center)\n",
    "\n",
    "    # === 组合权重 ===\n",
    "    v_combined = {\n",
    "        k: (1 - centrality_weight) * v_weights_norm.get(k, 0) + centrality_weight * v_center_norm.get(k, 0)\n",
    "        for k in used_vertices\n",
    "    }\n",
    "    e_combined = {\n",
    "        k: (1 - centrality_weight) * e_weights_norm.get(k, 0) + centrality_weight * e_center_norm.get(k, 0)\n",
    "        for k in used_edges\n",
    "    }\n",
    "\n",
    "    # === 分配价格 ===\n",
    "    remaining = new_price - total_cost\n",
    "    total_combined = sum(v_combined.values()) + sum(e_combined.values()) or 1e-6\n",
    "    v_ratio = {k: v / total_combined for k, v in v_combined.items()}\n",
    "    e_ratio = {k: v / total_combined for k, v in e_combined.items()}\n",
    "\n",
    "    for name in used_vertices:\n",
    "        base_cost = vertex_price_list[name]['cost']\n",
    "        alloc = remaining * v_ratio.get(name, 0)\n",
    "        vertex_price_list[name]['price'] = round(base_cost + alloc, 4)\n",
    "\n",
    "    for edge in used_edges:\n",
    "        base_cost = edge_price_list[edge]['weight']\n",
    "        alloc = remaining * e_ratio.get(edge, 0)\n",
    "        edge_price_list[edge]['price'] = round(base_cost + alloc, 4)\n",
    "\n",
    "    return vertex_price_list, edge_price_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transactions(\n",
    "    queries,\n",
    "    subgraphs,\n",
    "    vertex_price_list,\n",
    "    edge_price_list,\n",
    "    transaction_manager,\n",
    "    full_transaction,\n",
    "    plist_buyer,\n",
    "    evaluator,\n",
    "    start_time,\n",
    "    xpricing=1,\n",
    "    centrality_file=None\n",
    "):\n",
    "    evaluation_intervals = {10000, 30000, 50000, 80000, 120000, 150000}\n",
    "    results = {}\n",
    "\n",
    "    for i, Q in enumerate(queries, start=1):\n",
    "        expected_price = evaluator.generate_expected_price(Q)\n",
    "\n",
    "        # 更新或添加预期价格记录\n",
    "        found = False\n",
    "        for p_b in plist_buyer:\n",
    "            if gequal(Q, p_b[0]):\n",
    "                p_b[1] = expected_price\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            plist_buyer.append([Q, expected_price])\n",
    "\n",
    "        # --- Pricing Section ---\n",
    "        if xpricing == 0:\n",
    "            # 子图匹配定价\n",
    "            price, _ = subisomorphic_pricing(Q, subgraphs, vertex_price_list, edge_price_list)\n",
    "            success = expected_price >= price\n",
    "            print(f'第{i}次交易：{\"成功\" if success else \"失败\"} - Q: {Q}, 价格: {price}, 预期价格: {expected_price}')\n",
    "        else:\n",
    "            record = transaction_manager.get_summary(Q)\n",
    "\n",
    "            if record:\n",
    "                cost = computecost_G(Q, vertex_price_list, edge_price_list)\n",
    "                price = calculate_query_price(Q, vertex_price_list, edge_price_list)\n",
    "\n",
    "                sp_max = record['max_success_price']\n",
    "                fp_min = record['min_fail_price']\n",
    "                success_count = record['success_count']\n",
    "                fail_count = record['fail_count']\n",
    "\n",
    "                new_price = price\n",
    "\n",
    "                if expected_price < price:\n",
    "                    success = False\n",
    "                    print(f'第{i}次交易：失败 - Q: {Q}, 价格: {price}, 预期价格: {expected_price}')\n",
    "                    if success_count > 0:\n",
    "                        new_price = (sp_max + price) / 2\n",
    "                    else:\n",
    "                        diff = price - cost\n",
    "                        if diff > xpricing:\n",
    "                            new_price = price - xpricing\n",
    "                        elif diff > (xpricing / 2):\n",
    "                            new_price = price - xpricing / 2\n",
    "                        elif diff > 1:\n",
    "                            new_price = price - 1\n",
    "                        else:\n",
    "                            new_price = cost + 1\n",
    "                else:\n",
    "                    success = True\n",
    "                    print(f'第{i}次交易：成功 - Q: {Q}, 价格: {price}, 预期价格: {expected_price}')\n",
    "                    if fail_count > 0 and fp_min != float('inf'):\n",
    "                        new_price = (fp_min + price) / 2\n",
    "                    else:\n",
    "                        new_price = price + xpricing\n",
    "\n",
    "                if not isinstance(new_price, (int, float)) or not (new_price < float('inf')):\n",
    "                    print(\"Error: 非法 new_price，跳过 Q\")\n",
    "                    continue\n",
    "\n",
    "                if new_price != price:\n",
    "                    vertex_freq_dict, edge_freq_dict, vertex_succ_dict, edge_succ_dict = compute_frequency_and_success_rate(Q, full_transaction.get_history())\n",
    "                    vertex_centrality_dict, edge_centrality_dict = calculate_centrality(Q, centrality_file)\n",
    "                    vertex_price_list, edge_price_list = update_prices_advanced(\n",
    "                        Q,\n",
    "                        vertex_price_list,\n",
    "                        edge_price_list,\n",
    "                        vertex_freq_dict,\n",
    "                        vertex_succ_dict,\n",
    "                        edge_freq_dict,\n",
    "                        edge_succ_dict,\n",
    "                        vertex_centrality_dict,\n",
    "                        edge_centrality_dict,\n",
    "                        new_price\n",
    "                    )\n",
    "\n",
    "                price = new_price\n",
    "\n",
    "            else:\n",
    "                # 无历史记录：先进行子图匹配定价\n",
    "                price, _ = subisomorphic_pricing(Q, subgraphs, vertex_price_list, edge_price_list)\n",
    "                success = expected_price >= price\n",
    "                print(f'第{i}次交易：{\"成功\" if success else \"失败\"} - Q: {Q}, 价格: {price}, 预期价格: {expected_price}')\n",
    "\n",
    "                # 无历史记录也进行价格更新\n",
    "                new_price = expected_price if success else price\n",
    "                vertex_freq_dict, edge_freq_dict, vertex_succ_dict, edge_succ_dict = calculate_transaction_metrics(Q, full_transaction.get_history())\n",
    "                vertex_centrality_dict, edge_centrality_dict = calculate_centrality(Q, centrality_file)\n",
    "                vertex_price_list, edge_price_list = update_prices_advanced(\n",
    "                    Q,\n",
    "                    vertex_price_list,\n",
    "                    edge_price_list,\n",
    "                    vertex_freq_dict,\n",
    "                    vertex_succ_dict,\n",
    "                    edge_freq_dict,\n",
    "                    edge_succ_dict,\n",
    "                    vertex_centrality_dict,\n",
    "                    edge_centrality_dict,\n",
    "                    new_price\n",
    "                )\n",
    "\n",
    "        # 记录交易\n",
    "        transaction_manager.add_transaction(Q, price, success)\n",
    "        full_transaction.add_transaction(Q, price, success)\n",
    "\n",
    "        # 定期评估并保存结果\n",
    "        if i in evaluation_intervals:\n",
    "            e_time = time.time() - start_time\n",
    "            metrics = evaluator.evaluate_transactions(full_transaction.get_history())\n",
    "            results = {\n",
    "                \"success_ratio\": metrics[0],\n",
    "                \"avg_regret\": metrics[1],\n",
    "                \"avg_price_deviation\": metrics[2],\n",
    "                \"avg_sdiff\": metrics[3],\n",
    "                \"avg_diff\": metrics[4],\n",
    "                \"avg_per_diff\": metrics[5],\n",
    "                \"customer_avg_per_diff\": metrics[6],\n",
    "                \"time\": e_time\n",
    "            }\n",
    "            evaluator.save_evaluation_results(i, \"TEST_x0_test_evaluation_results.txt\", results)\n",
    "\n",
    "    # 最终评估\n",
    "    e_time = time.time() - start_time\n",
    "    metrics = evaluator.evaluate_transactions(full_transaction.get_history())\n",
    "    results = {\n",
    "        \"success_ratio\": metrics[0],\n",
    "        \"avg_regret\": metrics[1],\n",
    "        \"avg_price_deviation\": metrics[2],\n",
    "        \"avg_sdiff\": metrics[3],\n",
    "        \"avg_diff\": metrics[4],\n",
    "        \"avg_per_diff\": metrics[5],\n",
    "        \"customer_avg_per_diff\": metrics[6],\n",
    "        \"time\": e_time\n",
    "    }\n",
    "    print(results)\n",
    "    evaluator.save_evaluation_results(i, \"TEST_x0_test_evaluation_results.txt\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#熵值法\n",
    "#1.归一化所有属性值\n",
    "def min_max_normalize(data_dict):\n",
    "    values = list(data_dict.values())\n",
    "    min_val, max_val = min(values), max(values)\n",
    "    if max_val == min_val:\n",
    "        return {k: 0.0 for k in data_dict}  # 防止除0\n",
    "    return {k: (v - min_val) / (max_val - min_val) for k, v in data_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#熵值法\n",
    "#2.对交易频率和成功率使用熵值法确定权重\n",
    "def entropy_weight_method(attributes_list):\n",
    "    \"\"\"\n",
    "    attributes_list: List[Dict[id, value]]，每项代表一个属性的归一化结果\n",
    "\n",
    "    返回：\n",
    "    - List[float]，每个属性的权重\n",
    "    \"\"\"\n",
    "    ids = attributes_list[0].keys()\n",
    "    data = np.array([[attr[i] for i in ids] for attr in attributes_list], dtype=float)\n",
    "    eps = 1e-10\n",
    "\n",
    "    # 第一步：归一化\n",
    "    P = data / (data.sum(axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # 第二步：计算熵值\n",
    "    E = -np.sum(P * np.log(P + eps), axis=1) / np.log(len(ids))\n",
    "\n",
    "    # 第三步：计算权重\n",
    "    d = 1 - E\n",
    "    weights = d / d.sum()\n",
    "    return weights.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 归一化\n",
    "freq_norm = min_max_normalize(vertex_tf)\n",
    "tsr_norm = min_max_normalize(vertex_tsr)\n",
    "\n",
    "# Step 2: 熵值法动态计算 w1, w2\n",
    "w1, w2 = entropy_weight_method([freq_norm, tsr_norm])\n",
    "\n",
    "# Step 3: 设置中心性静态权重 w3\n",
    "w3 = 0.3  # 可设为 0.2～0.4，根据经验调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "for node in query_graph.nodes:\n",
    "    score_dict[node] = (\n",
    "        w1 * freq_norm.get(node, 0.0) +\n",
    "        w2 * tsr_norm.get(node, 0.0) +\n",
    "        w3 * centrality_norm.get(node, 0.0)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
